{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q07w1RrHB0T"
      },
      "source": [
        "# Identifying Bees Using Crowd Sourced Data using Amazon SageMaker\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8nXVsREHB0V"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsYz30x5HB0X"
      },
      "source": [
        "The archive contains the following structure: 500 `.jpg` image files, a manifest file (to be explained later) and 10 test images in the `test` subfolder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gUB_CG3HB0X"
      },
      "outputs": [],
      "source": [
        "!unzip -l dataset.zip | tail -20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFEAwFSbHB0X"
      },
      "source": [
        "Now let's upload this dataset to your own S3 bucket in preparation for labeling and training using Amazon SageMaker. For this demo, we will be using `us-west-2` region, so your bucket needs to be in this region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG1now6fHB0Y"
      },
      "outputs": [],
      "source": [
        "# S3 bucket must be created in us-west-2 (Oregon) region\n",
        "BUCKET = 'denisb-sagemaker-oregon'\n",
        "PREFIX = 'input' # this is the root path to your working space, feel to use a different path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usidSD5jHB0Y"
      },
      "outputs": [],
      "source": [
        "!aws s3 sync --exclude=\"*\" --include=\"[0-9]*.jpg\" . s3://$BUCKET/$PREFIX/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKpD9fu4HB0Y"
      },
      "source": [
        "## Labeling with SageMaker Ground Truth <a name=\"groundtruth\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAoLtHnlHB0Y"
      },
      "source": [
        "When specifying information needed to configure the labeling UI tool, use the following information:\n",
        "\n",
        "- Brief task description: _\"Draw a bounding box around the bee in this image.\"_\n",
        "- Labels: _\"bee\"_\n",
        "- Good example description: _\"bounding box includes all visible parts of the insect - legs, antennae, etc.\"_\n",
        "- Bad example description: _\"bounding box is too big and/or excludes some visible parts of the insect\"_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYay171SHB0Y"
      },
      "source": [
        "## Reviewing labeling results\n",
        "<a name=\"reviewing\"></a>\n",
        "\n",
        "After the labeling job has completed, we can see the results of image annotations right in the SageMaker console itself. The console displays each image as well as the bounding boxes around the bees that were drawn by human labelers.\n",
        "\n",
        "At the same time we can examine the results in the so-called augmented manifest file that was generated. Let's download and examine the manifest file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxKYCsCTHB0Z"
      },
      "outputs": [],
      "source": [
        "# Enter the name of your job here\n",
        "labeling_job_name = 'bee-detect'\n",
        "\n",
        "import boto3\n",
        "## A low level client representing Amazon SageMaker\n",
        "## Only using this, creating and managing SageMaker resources can be done. Provides API\n",
        "client = boto3.client('sagemaker')\n",
        "\n",
        "## Describing the Labelling Job\n",
        "# LabellingJobName: Name of the job created at Ground truth\n",
        "# Returns the JSON file, from that JSON file. Only S3OutputPath from OutputConfig Key has been taken\n",
        "s3_output = client.describe_labeling_job(LabelingJobName=labeling_job_name)['OutputConfig']['S3OutputPath'] + labeling_job_name\n",
        "augmented_manifest_url = f'{s3_output}/manifests/output/output.manifest'\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "# Shutil is used to replicate the Complete Directory\n",
        "# Shutil.rmtree is used to delete an entire directory tree\n",
        "try:\n",
        "    os.makedirs('od_output_data/', exist_ok=False)\n",
        "except FileExistsError:\n",
        "    shutil.rmtree('od_output_data/')\n",
        "\n",
        "# now download the augmented manifest file and display first 3 lines\n",
        "!aws s3 cp $augmented_manifest_url od_output_data/\n",
        "augmented_manifest_file = 'od_output_data/output.manifest'\n",
        "!head -3 $augmented_manifest_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4TWTEQPHB0Z"
      },
      "source": [
        "Now let's plot all the annotated images. First, let's define a function that displays the local image file and draws over it the bounding boxes obtained via labeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1xJy_BFHB0Z"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from itertools import cycle\n",
        "\n",
        "def show_annotated_image(img_path, bboxes):\n",
        "    im = np.array(Image.open(img_path), dtype=np.uint8)\n",
        "\n",
        "    # Create figure and axes\n",
        "    fig,ax = plt.subplots(1)\n",
        "\n",
        "    # Display the image\n",
        "    ax.imshow(im)\n",
        "\n",
        "    colors = cycle(['r', 'g', 'b', 'y', 'c', 'm', 'k', 'w'])\n",
        "\n",
        "    for bbox in bboxes:\n",
        "        # Create a Rectangle patch\n",
        "        rect = patches.Rectangle((bbox['left'],bbox['top']),bbox['width'],bbox['height'],linewidth=1,edgecolor=next(colors),facecolor='none')\n",
        "\n",
        "        # Add the patch to the Axes\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH5MavaIHB0a"
      },
      "source": [
        "Next, read the augmented manifest (JSON lines format) line by line and display the first 10 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAzZLmUzHB0a"
      },
      "outputs": [],
      "source": [
        "!pip -q install --upgrade pip\n",
        "!pip -q install jsonlines\n",
        "import jsonlines\n",
        "from itertools import islice\n",
        "# Returns specific element from the passed iterator\n",
        "\n",
        "with jsonlines.open(augmented_manifest_file, 'r') as reader:\n",
        "    for desc in islice(reader, 10):\n",
        "        img_url = desc['source-ref']\n",
        "        img_file = os.path.basename(img_url)\n",
        "        file_exists = os.path.isfile(img_file)\n",
        "\n",
        "        bboxes = desc[labeling_job_name]['annotations']\n",
        "        show_annotated_image(img_file, bboxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-gN43kyHB0a"
      },
      "source": [
        "<a name='training'></a>\n",
        "## Training an Object Detection Model\n",
        "We are now ready to use the labeled dataset in order to train a Machine Learning model using the SageMaker [built-in Object Detection algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html).\n",
        "\n",
        "For this, we would need to split the full labeled dataset into a training and a validation datasets. Out of the total of 500 images we are going to use 400 for training and 100 for validation. The algorithm will use the first one to train the model and the latter to estimate the accuracy of the model, trained so far. The augmented manifest file from the previously run full labeling job was included in the original zip archive as `output.manifest`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6E-XCM41HB0a"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with jsonlines.open('output.manifest', 'r') as reader:\n",
        "    lines = list(reader)\n",
        "    # Shuffle data in place.\n",
        "    np.random.shuffle(lines)\n",
        "\n",
        "dataset_size = len(lines)\n",
        "num_training_samples = round(dataset_size*0.8)\n",
        "\n",
        "train_data = lines[:num_training_samples]\n",
        "validation_data = lines[num_training_samples:]\n",
        "\n",
        "augmented_manifest_filename_train = 'train.manifest'\n",
        "\n",
        "with open(augmented_manifest_filename_train, 'w') as f:\n",
        "    for line in train_data:\n",
        "        f.write(json.dumps(line))\n",
        "        f.write('\\n')\n",
        "\n",
        "augmented_manifest_filename_validation = 'validation.manifest'\n",
        "\n",
        "with open(augmented_manifest_filename_validation, 'w') as f:\n",
        "    for line in validation_data:\n",
        "        f.write(json.dumps(line))\n",
        "        f.write('\\n')\n",
        "\n",
        "print(f'training samples: {num_training_samples}, validation samples: {len(lines)-num_training_samples}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au6leGpSHB0a"
      },
      "source": [
        "Next, let's upload the two manifest files to S3 in preparation for training. We will use the same bucket you created earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2rFHC0hHB0b"
      },
      "outputs": [],
      "source": [
        "pfx_training = PREFIX + '/training' if PREFIX else 'training'\n",
        "# Defines paths for use in the training job request.\n",
        "s3_train_data_path = 's3://{}/{}/{}'.format(BUCKET, pfx_training, augmented_manifest_filename_train)\n",
        "s3_validation_data_path = 's3://{}/{}/{}'.format(BUCKET, pfx_training, augmented_manifest_filename_validation)\n",
        "# Path of s3 bucket - for training data and validation data\n",
        "\n",
        "!aws s3 cp train.manifest s3://$BUCKET/$pfx_training/\n",
        "!aws s3 cp validation.manifest s3://$BUCKET/$pfx_training/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT2TiPxQHB0b"
      },
      "source": [
        "We are now ready to kick off the training. We will do it from the SageMaker console, but alternatively, you can just run this code in a new cell using SageMaker Python SDK:\n",
        "### Code option\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaTkxPPsHB0b"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import sagemaker\n",
        "\n",
        "role = sagemaker.get_execution_role()\n",
        "# Get the execution role for the notebook instance. This is the IAM role that we created for your notebook instance.\n",
        "# We pass the role to the tuning job\n",
        "sess = sagemaker.Session()\n",
        "# Managing interactions with the Amazon SageMaker API's and any other AWS Services needed.\n",
        "# Session - class - provides convenient methods for manipulating entities and resources that Amazon SageMaker uses, such as training jobs, endpoints, and input datasets in s3.\n",
        "\n",
        "\n",
        "training_image = sagemaker.amazon.amazon_estimator.get_image_uri(\n",
        "    boto3.Session().region_name, 'object-detection', repo_version='latest')\n",
        "s3_output_path = 's3://{}/{}/output'.format(BUCKET, pfx_training)\n",
        "\n",
        "# This is done manually using the code\n",
        "\n",
        "\n",
        "# Create unique job name\n",
        "training_job_name = 'bees-detection-resnet'\n",
        "\n",
        "## Provide the list of parameters\n",
        "# More than that, the same hyperparameters are given in the training job section in SageMaker console\n",
        "training_params = \\\n",
        "    {\n",
        "        \"AlgorithmSpecification\": {\n",
        "            # NB. This is one of the named constants defined in the first cell.\n",
        "            \"TrainingImage\": training_image,\n",
        "            \"TrainingInputMode\": \"Pipe\"\n",
        "        },\n",
        "        \"RoleArn\": role,\n",
        "        \"OutputDataConfig\": {\n",
        "            \"S3OutputPath\": s3_output_path\n",
        "        },\n",
        "        \"ResourceConfig\": {\n",
        "            \"InstanceCount\": 1,\n",
        "            \"InstanceType\": \"ml.p2.xlarge\",\n",
        "            \"VolumeSizeInGB\": 50\n",
        "        },\n",
        "        \"TrainingJobName\": training_job_name,\n",
        "        \"HyperParameters\": {  # NB. These hyperparameters are at the user's discretion and are beyond the scope of this demo.\n",
        "            \"base_network\": \"resnet-50\", # vgg-16 available as pre-trained model in object detection in Sagemaker training job\n",
        "            \"use_pretrained_model\": \"1\",\n",
        "            \"num_classes\": \"1\",\n",
        "            \"mini_batch_size\": \"1\",\n",
        "            \"epochs\": \"100\",\n",
        "            \"learning_rate\": \"0.001\",\n",
        "            \"lr_scheduler_step\": \"\",\n",
        "            \"lr_scheduler_factor\": \"0.1\",\n",
        "            \"optimizer\": \"sgd\",\n",
        "            \"momentum\": \"0.9\",\n",
        "            \"weight_decay\": \"0.0005\",\n",
        "            \"overlap_threshold\": \"0.5\",\n",
        "            \"nms_threshold\": \"0.45\",\n",
        "            \"image_shape\": \"300\",\n",
        "            \"label_width\": \"350\",\n",
        "            \"num_training_samples\": str(num_training_samples)\n",
        "        },\n",
        "        \"StoppingCondition\": {\n",
        "            \"MaxRuntimeInSeconds\": 86400\n",
        "        },\n",
        "        \"InputDataConfig\": [\n",
        "            # Creating seperate channels for training and validation\n",
        "            {\n",
        "                \"ChannelName\": \"train\",\n",
        "                \"DataSource\": {\n",
        "                    \"S3DataSource\": {\n",
        "                        \"S3DataType\": \"AugmentedManifestFile\",  # NB. Augmented Manifest\n",
        "                        \"S3Uri\": s3_train_data_path,\n",
        "                        \"S3DataDistributionType\": \"FullyReplicated\",\n",
        "                        # NB. This must correspond to the JSON field names in your augmented manifest.\n",
        "                        \"AttributeNames\": ['source-ref', 'bees-500']\n",
        "                    }\n",
        "                },\n",
        "                \"ContentType\": \"application/x-recordio\",\n",
        "                \"RecordWrapperType\": \"RecordIO\",\n",
        "                \"CompressionType\": \"None\"\n",
        "            },\n",
        "            {\n",
        "                \"ChannelName\": \"validation\",\n",
        "                \"DataSource\": {\n",
        "                    \"S3DataSource\": {\n",
        "                        \"S3DataType\": \"AugmentedManifestFile\",  # NB. Augmented Manifest\n",
        "                        \"S3Uri\": s3_validation_data_path,\n",
        "                        \"S3DataDistributionType\": \"FullyReplicated\",\n",
        "                        # NB. This must correspond to the JSON field names in your augmented manifest.\n",
        "                        \"AttributeNames\": ['source-ref', 'bees-500']\n",
        "                    }\n",
        "                },\n",
        "                \"ContentType\": \"application/x-recordio\",\n",
        "                \"RecordWrapperType\": \"RecordIO\",\n",
        "                \"CompressionType\": \"None\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "# Now we create the SageMaker training job.\n",
        "client = boto3.client(service_name='sagemaker')\n",
        "client.create_training_job(**training_params)\n",
        "\n",
        "# Confirm that the training job has started\n",
        "status = client.describe_training_job(TrainingJobName=training_job_name)['TrainingJobStatus']\n",
        "print('Training job current status: {}'.format(status))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hKWDRujHB0c"
      },
      "source": [
        "To check the progess of the training job, you can refresh the console or repeatedly evaluate the following cell. When the training job status reads `'Completed'`, move on to the next part of the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vbmapo8PHB0c"
      },
      "outputs": [],
      "source": [
        "##### REPLACE WITH YOUR OWN TRAINING JOB NAME\n",
        "# In the above console screenshots the job name was 'bees-detection-resnet'.\n",
        "# But if you used Python to kick off the training job,\n",
        "# then 'training_job_name' is already set, so you can comment out the line below.\n",
        "\n",
        "# This is done using training job section in Sagemaker console\n",
        "training_job_name = 'bees-training'\n",
        "##### REPLACE WITH YOUR OWN TRAINING JOB NAME\n",
        "\n",
        "training_info = client.describe_training_job(TrainingJobName=training_job_name)\n",
        "\n",
        "print(\"Training job status: \", training_info['TrainingJobStatus'])\n",
        "print(\"Secondary status: \", training_info['SecondaryStatus'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxXyVYfNHB0c"
      },
      "source": [
        "<a name='review_training'></a>\n",
        "\n",
        "## Review of Training Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzTxMjo1HB0c"
      },
      "source": [
        "First, let's create the SageMaker model out of model artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Atx7IGunHB0c"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
        "model_name = training_job_name + '-model' + timestamp\n",
        "\n",
        "# training image and modeldata are the two different attributes present in the primary container.\n",
        "\n",
        "training_image = training_info['AlgorithmSpecification']['TrainingImage']\n",
        "model_data = training_info['ModelArtifacts']['S3ModelArtifacts']\n",
        "\n",
        "primary_container = {\n",
        "    'Image': training_image,\n",
        "    'ModelDataUrl': model_data,\n",
        "}\n",
        "\n",
        "from sagemaker import get_execution_role\n",
        "\n",
        "role = get_execution_role()\n",
        "# Get the Iam role of the sageMaker\n",
        "\n",
        "#create the model using client = boto3.client(service_name = \"sagemaker\")\n",
        "create_model_response = client.create_model(\n",
        "    ModelName = model_name,\n",
        "    ExecutionRoleArn = role,\n",
        "    PrimaryContainer = primary_container)\n",
        "\n",
        "print(create_model_response['ModelArn'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAAZdU0mHB0c"
      },
      "outputs": [],
      "source": [
        "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
        "endpoint_config_name = training_job_name + '-epc' + timestamp\n",
        "endpoint_config_response = client.create_endpoint_config(\n",
        "    EndpointConfigName = endpoint_config_name,\n",
        "    ProductionVariants=[{\n",
        "        'InstanceType':'ml.t2.medium',\n",
        "        'InitialInstanceCount':1,\n",
        "        'ModelName':model_name,\n",
        "        'VariantName':'AllTraffic'}])\n",
        "\n",
        "print('Endpoint configuration name: {}'.format(endpoint_config_name))\n",
        "print('Endpoint configuration arn:  {}'.format(endpoint_config_response['EndpointConfigArn']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAB29PshHB0d"
      },
      "source": [
        "### Create Endpoint\n",
        "\n",
        "The next cell creates an endpoint that can be validated and incorporated into production applications. This takes about 10 minutes to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSjpsXfnHB0d"
      },
      "outputs": [],
      "source": [
        "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
        "endpoint_name = training_job_name + '-ep' + timestamp\n",
        "print('Endpoint name: {}'.format(endpoint_name))\n",
        "\n",
        "## Creating the endpoint for productionizing\n",
        "endpoint_params = {\n",
        "    'EndpointName': endpoint_name,\n",
        "    'EndpointConfigName': endpoint_config_name,\n",
        "}\n",
        "endpoint_response = client.create_endpoint(**endpoint_params)\n",
        "print('EndpointArn = {}'.format(endpoint_response['EndpointArn']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPxDJ-DoHB0d"
      },
      "outputs": [],
      "source": [
        "endpoint_name=\"test-tuning-job-008-9ff8af52-ep-2019-07-19-12-25-46\"\n",
        "# get the status of the endpoint\n",
        "response = client.describe_endpoint(EndpointName=endpoint_name)\n",
        "status = response['EndpointStatus']\n",
        "print('EndpointStatus = {}'.format(status))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OvNAIhjHB0d"
      },
      "source": [
        "### Perform inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lch6mOj0HB0d"
      },
      "source": [
        "We will invoke the deployed endpoint to detect bees in the 10 test images that were inside the `test` folder in `dataset.zip`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fO_pj2ZXHB0d"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "# This module finds all the pathnames matching a specified pattern according to the rules used bu the UNIX shell\n",
        "# Finds the path name globally -- here test finds globally\n",
        "test_images = glob.glob('test/*')\n",
        "print(*test_images, sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8NTDmKZHB0e"
      },
      "source": [
        "Next, define a function that converts the prediction array returned by our endpoint to the bounding box structure expected by our image display function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSEEkGWwHB0e"
      },
      "outputs": [],
      "source": [
        "def prediction_to_bbox_data(image_path, prediction):\n",
        "    class_id, confidence, xmin, ymin, xmax, ymax = prediction\n",
        "    width, height = Image.open(image_path).size\n",
        "    bbox_data = {'class_id': class_id,\n",
        "               'height': (ymax-ymin)*height,\n",
        "               'width': (xmax-xmin)*width,\n",
        "               'left': xmin*width,\n",
        "               'top': ymin*height}\n",
        "    return bbox_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RkhILZ5HB0e"
      },
      "source": [
        "Finally, for each of the test images, the following cell transforms the image into the appropriate format for realtime prediction, repeatedly calls the endpoint, receives back the prediction, and displays the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2D5IIZSoHB0e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "runtime_client = boto3.client('sagemaker-runtime')\n",
        "\n",
        "# Call SageMaker endpoint to obtain predictions\n",
        "def get_predictions_for_img(runtime_client, endpoint_name, img_path):\n",
        "    with open(img_path, 'rb') as f:\n",
        "        payload = f.read()\n",
        "        payload = bytearray(payload)\n",
        "\n",
        "    response = runtime_client.invoke_endpoint(EndpointName=endpoint_name,\n",
        "                                       ContentType='application/x-image',\n",
        "                                       Body=payload)\n",
        "\n",
        "    result = response['Body'].read()\n",
        "    result = json.loads(result)\n",
        "    return result\n",
        "\n",
        "\n",
        "# wait until the status has changed\n",
        "client.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
        "endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
        "status = endpoint_response['EndpointStatus']\n",
        "if status != 'InService':\n",
        "    raise Exception('Endpoint creation failed.')\n",
        "\n",
        "for test_image in test_images:\n",
        "    result = get_predictions_for_img(runtime_client, endpoint_name, test_image)\n",
        "    confidence_threshold = .2 ## Confidence threshold -- Keep as much as low as possible because the training job doesn't have much confidence and tries not to give exact bounding boxes to the images\n",
        "    best_n = 3\n",
        "    # display the best n predictions with confidence > confidence_threshold\n",
        "    predictions = [prediction for prediction in result['prediction'] if prediction[1] > confidence_threshold]\n",
        "    predictions.sort(reverse=True, key = lambda x: x[1])\n",
        "    bboxes = [prediction_to_bbox_data(test_image, prediction) for prediction in predictions[:best_n]]\n",
        "    show_annotated_image(test_image, bboxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERHS8YwIHB0g"
      },
      "outputs": [],
      "source": [
        "client.delete_endpoint(EndpointName=endpoint_name)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "conda_python3",
      "language": "python",
      "name": "conda_python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}